name: Update Job Listings Hourly (300 Latest + Cumulative Archive)

on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-and-update-jobs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Create data directory
        run: mkdir -p data

      - name: Fetch up to 300 latest jobs (3 pages of 100)
        run: |
          API="https://merorojgari.com/wp-json/wp/v2/job-listings"
          PER_PAGE=100
          PAGES=3  # For 300 jobs max

          # Fetch each page and collect into ndjson temp file
          > data/temp-jobs.ndjson  # Clear/create file

          for page in {1..3}; do
            echo "Fetching page $page..."
            curl -s --fail "$API?per_page=$PER_PAGE&page=$page&orderby=date&order=desc" > "data/page-$page.json"

            # Append individual jobs as lines (ndjson)
            jq -c '.[]' "data/page-$page.json" >> data/temp-jobs.ndjson || echo "Page $page empty or error"
          done

          # Combine into single sorted array (newest first, since API orders desc)
          if [ -s data/temp-jobs.ndjson ]; then
            jq -s '.' data/temp-jobs.ndjson > data/latest-300.json
          else
            echo "[]" > data/latest-300.json
          fi

          # Limit strictly to 300
          jq '.[0:300]' data/latest-300.json > data/temp-latest.json
          mv data/temp-latest.json data/latest-300.json

          # Cleanup
          rm -f data/page-*.json data/temp-jobs.ndjson

      - name: Update cumulative all-jobs.json (prepend new unique jobs)
        run: |
          node - <<'EOF'
          const fs = require('fs');

          const cumulativePath = 'data/all-jobs.json';
          const latestPath = 'data/latest-300.json';

          // Load existing cumulative archive
          let allJobs = [];
          if (fs.existsSync(cumulativePath)) {
            try {
              allJobs = JSON.parse(fs.readFileSync(cumulativePath, 'utf8'));
            } catch (e) {
              console.error('Corrupted all-jobs.json, starting fresh');
              allJobs = [];
            }
          }

          // Load freshly fetched latest jobs
          let latestJobs = [];
          try {
            latestJobs = JSON.parse(fs.readFileSync(latestPath, 'utf8'));
          } catch (e) {
            console.error('No latest jobs fetched');
            process.exit(0);
          }

          if (latestJobs.length === 0) {
            console.log('No jobs fetched');
            process.exit(0);
          }

          // Deduplicate: find jobs in latest not in cumulative
          const existingIds = new Set(allJobs.map(j => j.id));
          const newUniqueJobs = latestJobs.filter(j => !existingIds.has(j.id));

          let updated = false;
          if (newUniqueJobs.length > 0) {
            console.log(`Found ${newUniqueJobs.length} new unique jobs to add`);
            // Prepend so newest are first
            allJobs = [...newUniqueJobs, ...allJobs];
            updated = true;
          } else {
            console.log('No new unique jobs');
          }

          // Always write files (pretty formatted)
          fs.writeFileSync(cumulativePath, JSON.stringify(allJobs, null, 2));
          fs.writeFileSync(latestPath, JSON.stringify(latestJobs, null, 2));

          if (!updated) {
            console.log('No changes to cumulative archive');
          }
          EOF

      - name: Commit and push if changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/all-jobs.json data/latest-300.json

          if git diff --staged --quiet; then
            echo "No new jobs added â€“ no commit needed"
          else
            git commit -m "Hourly sync: added new job listings to archive"
            git push
          fi
