name: Update Sports Feed
on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch: 

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Fetch All Feed History and Convert
        run: |
          mkdir -p data
          python3 - <<EOF
          import xml.etree.ElementTree as ET
          import json
          import os
          import re
          import urllib.request

          def clean_text(text):
              if not text: return ""
              return re.sub('<[^<]+?>', '', text).strip()

          def fetch_page(page_num):
              url = f"https://english.onlinekhabar.com/category/sports/feed?paged={page_num}"
              try:
                  with urllib.request.urlopen(url) as response:
                      return response.read()
              except:
                  return None

          # 1. Load existing data
          file_path = 'data/sports.json'
          old_data = []
          if os.path.exists(file_path):
              with open(file_path, 'r') as f:
                  try:
                      old_data = json.load(f)
                  except: pass
          
          existing_ids = {post['id'] for post in old_data}
          newly_discovered = []
          
          # 2. Deep Fetch Loop
          page = 1
          while page <= 5: # Fetch up to 5 pages to find all missing data
              print(f"Fetching page {page}...")
              xml_data = fetch_page(page)
              if not xml_data: break
              
              root = ET.fromstring(xml_data)
              items = root.findall('./channel/item')
              
              found_new_on_this_page = False
              page_items = []
              
              for item in items:
                  link = item.find('link').text
                  if link not in existing_ids:
                      found_new_on_this_page = True
                      
                      # Extract Image from <figure> or media tag
                      content_tag = item.find('{http://purl.org/rss/1.0/modules/content/}encoded')
                      full_content = content_tag.text if content_tag is not None else ""
                      figure_match = re.search(r'<figure[^>]*class="wp-block-image[^>]*>.*?src="([^"]+)"', full_content, re.DOTALL)
                      
                      image = "https://via.placeholder.com/1080x1920?text=Sports"
                      if figure_match:
                          image = figure_match.group(1)

                      page_items.append({
                          "id": link,
                          "date": item.find('pubDate').text,
                          "title": {"rendered": item.find('title').text},
                          "excerpt": {"rendered": clean_text(item.find('description').text or "")[:150] + "..."},
                          "content": {"rendered": full_content},
                          "featured_image": image
                      })
              
              newly_discovered.extend(page_items)
              existing_ids.update([p['id'] for p in page_items])
              
              # If a page contains only items we already have, we can stop
              if not found_new_on_this_page:
                  break
              page += 1

          # 3. Merge: New on top, Old below
          final_json = newly_discovered + old_data
          
          # Keep the database healthy (max 500 items)
          with open(file_path, 'w') as f:
              json.dump(final_json[:500], f, indent=2)
          
          print(f"Added {len(newly_discovered)} new articles.")
          EOF

      - name: Commit Changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/sports.json
          git commit -m "Deep Update sports.json" || exit 0
          git push
